{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0dc14b",
   "metadata": {},
   "source": [
    "# STANZA LEMMATIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d0c08",
   "metadata": {},
   "source": [
    "### IMPORTS, VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad9ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "import stanza\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c17604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_xpos = '../data/memoirs_3k_corrected.conllu'\n",
    "file_upos = '../data/memoirs_10k_corrected.conllu'\n",
    "test_file = '../data/ud-treebanks/UD_Polish-PDB/pl_pdb-ud-test.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b775fee",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91bbc23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stanza_anns(sentences: list, processors: str, tag_type: str):\n",
    "    '''A function that obtains and processes Stanza lemmatization annotations.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): A list of lists of tokenized sentences.\n",
    "        processors (str): The kind of processing that is desired as per Stanza documentation\n",
    "        tag_type (str): The type of tag that should get retrieved.\n",
    "        \n",
    "    Returns:\n",
    "        A list lists representing the Stanza lemmatization annotations.\n",
    "    '''\n",
    "    # defining the stanza pipeline\n",
    "    nlp = stanza.Pipeline(lang='pl', processors=processors, tokenize_pretokenized=True)\n",
    "    # getting stanza annotations\n",
    "    annotations = []\n",
    "    for i, sent in enumerate(tqdm(sentences, desc='Retrieving annotations per sentence...')):\n",
    "        sent = ' '.join(sent)\n",
    "        pred = nlp(sent)\n",
    "        sent_annotations = []\n",
    "        # getting out the lemmas\n",
    "        for entry in pred.to_dict()[0]:\n",
    "            sent_annotations.append(entry[tag_type])\n",
    "        annotations.append(sent_annotations)\n",
    "            \n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00c628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ranges(tokens: list):\n",
    "    '''A function that removes the \"range\" elements in a list based off of a conllu file. This is so as to\n",
    "    exclude situations where in the list of tokens in a sentence one gets both \"zrobiłem\" and \"zrobił\" + \"em\".\n",
    "    \n",
    "    Args:\n",
    "        tokens (list[str]): A list of token-tag pairs.\n",
    "\n",
    "    Returns:\n",
    "        A list of token-tag pairs with the elements without a tag (with \"_\" instead of it) are excluded.\n",
    "    '''\n",
    "    tokens = [x for x in tokens if ' _' not in x]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a504f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conllu_data(filename: str, feature: str, sentences: bool = True, combined: bool = False, fulltext: bool = True):\n",
    "    '''A function that allows for the extraction of the desired data from a conllu file, structured into sentences or not.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The name of the .conllu file to be read.\n",
    "        feature (str): The name of the desired conllu format feature.\n",
    "        sentences (bool): Whether or not the output should be a list of lists of strings representing words in separate sentences.\n",
    "        combined (bool): Whether or not the tokens and tags should be returned in one list of space-separated elements.\n",
    "        fulltext (bool): Whether or not to extract and return the metadata sentences.\n",
    "        \n",
    "    Returns:\n",
    "        A list of the original tokens (tokenized sentences), a list of the corresponding features, and a list of full original \n",
    "        sentences.\n",
    "    '''\n",
    "    #checking the validity of the feature argument\n",
    "    possible_features = ['lemma', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
    "    if feature not in possible_features:\n",
    "        print('Please specify a valid feature type.')\n",
    "        return\n",
    "    \n",
    "    # specifying lists\n",
    "    tokens_features = []\n",
    "    tokens = []\n",
    "    features = []\n",
    "    data = []\n",
    "    \n",
    "    # opening the file\n",
    "    with open(filename) as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # parsing the file\n",
    "    sents = conllu.parse(text)\n",
    "    \n",
    "    # selecting the relevant data and adding it to relevant lists\n",
    "    for sentence in sents:\n",
    "        if fulltext:\n",
    "            data.append(sentence.metadata['text'])\n",
    "        sent_tokens_features = []\n",
    "        sent_tokens = []\n",
    "        sent_features = []\n",
    "        for entry in sentence:\n",
    "            token = entry['form']\n",
    "            feat = entry[feature]\n",
    "            \n",
    "            sent_tokens.append(token)\n",
    "            sent_features.append(feat)\n",
    "            \n",
    "            if combined:  # this will return a different data structure\n",
    "                if feat == None:\n",
    "                    feat = '_'\n",
    "                sent_tokens_features.append(' '.join([token, feat]))\n",
    "            \n",
    "            \n",
    "        tokens.append(sent_tokens)\n",
    "        features.append(sent_features)\n",
    "        \n",
    "        if combined:\n",
    "            tokens_features.append(sent_tokens_features)\n",
    "    \n",
    "    # unravelling the sentence-level lists if needed\n",
    "    if not sentences:\n",
    "        tokens = [x for sentence in tokens for x in sentence]\n",
    "        features = [x for sentence in features for x in sentence]\n",
    "        if combined:\n",
    "            tokens_features = [x for sentence in tokens_features for x in sentence]\n",
    "            \n",
    "    if combined:\n",
    "        if fulltext:\n",
    "            return tokens_features, data\n",
    "        else:\n",
    "            return tokens_features\n",
    "    else:\n",
    "        if fulltext:        \n",
    "            return tokens, features, data\n",
    "        else:\n",
    "            return tokens, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47ee60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measures(gold_standard: list, predictions: list, labels: list = [], matrix: bool = False, details: bool = False):\n",
    "    '''A function intended for retrieving a selection of evaluation measures for comparing the gold standard and the tagger\n",
    "    annotations. The measures are printed out and include accuracy, Matthew's Correlation Coefficient, per-class precision \n",
    "    and recall, as well as a confusion matrix, which, in addition, get saved locally. These measures are calculated using \n",
    "    functions from sklearn and pyplot.\n",
    "    \n",
    "    Args:\n",
    "        gold_standard (list[str]): A list of gold standard labels.\n",
    "        predictions (list[str]): A list of predicted labels.\n",
    "        labels (list[str]): A list of labels (if it needs to be specified).\n",
    "        matrix (bool): Whether or not to produce a confusion matrix.\n",
    "    '''\n",
    "    \n",
    "    if isinstance(gold_standard[0], list):\n",
    "        gold_standard = [x for sentence in gold_standard for x in sentence]\n",
    "    if isinstance(predictions[0], list):\n",
    "        predictions = [x for sentence in predictions for x in sentence]\n",
    "\n",
    "    if labels == []:  # setting up a list of labels based on the training data\n",
    "        labels = sorted(list(set(gold_standard)))\n",
    "    else:\n",
    "        if isinstance(labels[0], list):\n",
    "            labels = [x for sentence in labels for x in sentence]\n",
    "\n",
    "    # printing out the measures\n",
    "    print('MEASURES:')\n",
    "    print(f'Accuracy: {\"{:.2%}\".format(sklearn.metrics.accuracy_score(gold_standard, predictions))}')\n",
    "    print(f'Precision (weighted): {\"{:.2%}\".format(sklearn.metrics.precision_score(gold_standard, predictions, average=\"weighted\", zero_division=0))}')\n",
    "    print(f'Recall (weighted): {\"{:.2%}\".format(sklearn.metrics.recall_score(gold_standard, predictions, average=\"weighted\", zero_division=0))}')\n",
    "    print(f'F1 (weighted): {\"{:.2%}\".format(sklearn.metrics.f1_score(gold_standard, predictions, average=\"weighted\", zero_division=0))}')\n",
    "    print(f'Matthew\\'s Correlation Coefficient: {\"{:.2%}\".format(sklearn.metrics.matthews_corrcoef(gold_standard, predictions))}')\n",
    "    if details:\n",
    "        print()\n",
    "        print('MEASURES PER CLASS:')\n",
    "        precision = sklearn.metrics.precision_score(gold_standard, predictions, average=None, labels=labels, zero_division=0)\n",
    "        print('Precision:')\n",
    "        for i in range(0,len(labels)):\n",
    "            print(f'\\t{labels[i]}: {\"{:.2%}\".format(precision[i])}')\n",
    "        recall = sklearn.metrics.recall_score(gold_standard, predictions, average=None, labels=labels, zero_division=0)\n",
    "        print('Recall:')\n",
    "        for i in range(0,len(labels)):\n",
    "            print(f'\\t{labels[i]}: {\"{:.2%}\".format(recall[i])}')\n",
    "        print()\n",
    "    \n",
    "    # printing out and saving the confusion matrix\n",
    "    if matrix:\n",
    "        print('Confusion matrix:')\n",
    "        cm = sklearn.metrics.confusion_matrix(gold_standard, predictions)\n",
    "        matrix = sklearn.metrics.ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "        fig, ax = plt.subplots(figsize=(12,12))\n",
    "        matrix.plot(ax=ax)\n",
    "        \n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        plt.savefig(timestr + \"confusion_matrix.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755a9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparison(standard: list, predictions: list, tokens: list, confidence=[]):\n",
    "    '''A function that returns a comparison of where mistakes were made during annotation.\n",
    "    \n",
    "    Args:\n",
    "        standard (list): A list of gold standard annotations.\n",
    "        predictions (list): A list of predicted annotations.\n",
    "        tokens (list): A list of original tokens corresponding to the tags.\n",
    "    \n",
    "    Returns:\n",
    "        A Pandas dataframe containing the mismatched annotations, their context and tokens.\n",
    "    '''\n",
    "\n",
    "    if isinstance(standard[0], list):\n",
    "        standard = [x for sentence in standard for x in sentence]\n",
    "    if isinstance(predictions[0], list):\n",
    "        predictions = [x for sentence in predictions for x in sentence]\n",
    "    if isinstance(tokens[0], list):\n",
    "        tokens = [x for sentence in tokens for x in sentence]\n",
    "    \n",
    "    problematic = []\n",
    "    for i, ann in enumerate(predictions):\n",
    "        if standard[i] != ann:\n",
    "            if i != 0:\n",
    "                preceding = tokens[i-1]\n",
    "            else:\n",
    "                preceding = ''\n",
    "                \n",
    "            if i != len(tokens)-1:\n",
    "                succeeding = tokens[i+1]\n",
    "            else:\n",
    "                succeeding = ''\n",
    "            \n",
    "            if not confidence:\n",
    "                problematic.append((tokens[i], ' '.join([preceding, tokens[i], succeeding]), standard[i], predictions[i]))\n",
    "            else:\n",
    "                if isinstance(confidence[0], list):\n",
    "                    confidence = [x for sentence in confidence for x in sentence]\n",
    "                problematic.append((tokens[i], ' '.join([preceding, tokens[i], succeeding]), standard[i], predictions[i], confidence[i]))\n",
    "    if not confidence:        \n",
    "        problematic_frame = pd.DataFrame(problematic, columns=['Token', 'Context', 'Gold Standard', 'Prediction'])\n",
    "    else:\n",
    "        problematic_frame = pd.DataFrame(problematic, columns=['Token', 'Context', 'Gold Standard', 'Prediction', 'Confidence'])\n",
    "    \n",
    "    return problematic_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca747c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_measures(standard: list, predictions: list, lowercase: bool = False):\n",
    "    '''A function that calculates and prints out the accuracy of the lemmatization.\n",
    "    \n",
    "    Args:\n",
    "        standard (list): A list of lists of gold standard lemmas.\n",
    "        predictions (list): A list of lists of predicted lemmas.\n",
    "        lowercase (bool): Whether or not both of the lists should be lowercased.\n",
    "    '''\n",
    "    if lowercase:\n",
    "        standard = [x.lower() for sentence in standard for x in sentence]\n",
    "        predictions = [x.lower() for sentence in predictions for x in sentence]\n",
    "    else:\n",
    "        standard = [x for sentence in standard for x in sentence]\n",
    "        predictions = [x for sentence in predictions for x in sentence]\n",
    "\n",
    "    print(f'Accuracy: {\"{:.2%}\".format(sklearn.metrics.accuracy_score(standard, predictions))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91463959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_comparison(standard: list, predictions: list, tokens: list, lowercase: bool = False):\n",
    "    '''A function that calculates and prints out the accuracy of the lemmatization.\n",
    "    \n",
    "    Args:\n",
    "        standard (list): A list of lists of gold standard lemmas.\n",
    "        predictions (list): A list of lists of predicted lemmas.\n",
    "        tokens (list): A list of lists of the tokens to be lemmatized.\n",
    "        lowercase (bool): Whether or not standard and predictions should be lowercased.\n",
    "    \n",
    "    Returns:\n",
    "        A Pandas dataframe containing the mismatched lemmas.\n",
    "    '''\n",
    "    tokens = [x for sentence in tokens for x in sentence]\n",
    "\n",
    "    if lowercase:\n",
    "        standard = [x.lower() for sentence in standard for x in sentence]\n",
    "        predictions = [x.lower() for sentence in predictions for x in sentence]\n",
    "    else:\n",
    "        standard = [x for sentence in standard for x in sentence]\n",
    "        predictions = [x for sentence in predictions for x in sentence]\n",
    "            \n",
    "    problematic_frame = get_comparison(standard, predictions, tokens)\n",
    "    \n",
    "    return problematic_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8345ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tagger_friendly(tokens_tags):\n",
    "    '''A function allowing for the use of split_tags_and_tokens and remove_ranges on nested lists.\n",
    "    \n",
    "    Arguments:\n",
    "        token_tags (list[list]): A list of lists representing sentences with annotations.\n",
    "        \n",
    "    Returns:\n",
    "        Two separate lists of lists representing sentences and their annotations respectively.'''\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for element in tokens_tags:\n",
    "        mini_tokens, mini_tags = split_tags_and_tokens(remove_ranges(element))\n",
    "        tokens.append(mini_tokens)\n",
    "        tags.append(mini_tags)\n",
    "        \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e04d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tags_and_tokens(tags: list):\n",
    "    '''A function that splits every entry in a list by whitespace and into two separate lists.\n",
    "    \n",
    "    Args:\n",
    "        tags (list): A list where every entry is a string containing whitespace.\n",
    "        \n",
    "    Returns:\n",
    "        Two lists, containing the first and the second element of every entry from the original list.\n",
    "    '''\n",
    "    tokens = [x.strip().split()[0] for x in tags if len(x.strip()) > 1]\n",
    "    tags = [(' ').join(x.strip().split()[1:]) for x in tags if len(x.strip()) > 1]\n",
    "\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a4ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_table(standard: list, predictions: list, tokens: list, confidence=[], lowercase: bool = False):\n",
    "    '''A function that returns a list of all the tokens with their predictions, gold standard, and context.\n",
    "    \n",
    "    Args:\n",
    "        standard (list): A list of gold standard annotations.\n",
    "        predictions (list): A list of predicted annotations.\n",
    "        tokens (list): A list of original tokens corresponding to the tags.\n",
    "        confidence (list): A list of prediction confidences, if available; empty by default.\n",
    "        lowercase (bool): Whether or not standard and predictions should be lowercased.\n",
    "    \n",
    "    Returns:\n",
    "        A Pandas dataframe containing the mismatched annotations, their context and tokens.\n",
    "    '''\n",
    "\n",
    "    if isinstance(standard[0], list):\n",
    "        standard = [x for sentence in standard for x in sentence]\n",
    "    if isinstance(predictions[0], list):\n",
    "        predictions = [x for sentence in predictions for x in sentence]\n",
    "    if isinstance(tokens[0], list):\n",
    "        tokens = [x for sentence in tokens for x in sentence]\n",
    "\n",
    "    if lowercase:\n",
    "        standard = [x.lower() for x in standard]\n",
    "        predictions = [x.lower() for x in predictions]\n",
    "    \n",
    "    all_entries = []\n",
    "    for i, ann in enumerate(predictions):\n",
    "        if i != 0:\n",
    "            preceding = tokens[i-1]\n",
    "        else:\n",
    "            preceding = ''\n",
    "                \n",
    "        if i != len(tokens)-1:\n",
    "            succeeding = tokens[i+1]\n",
    "        else:\n",
    "            succeeding = ''\n",
    "            \n",
    "        if not confidence:\n",
    "            all_entries.append((tokens[i], ' '.join([preceding, tokens[i], succeeding]), standard[i], predictions[i]))\n",
    "        else:\n",
    "            if isinstance(confidence[0], list):\n",
    "                confidence = [x for sentence in confidence for x in sentence]\n",
    "            all_entries.append((tokens[i], ' '.join([preceding, tokens[i], succeeding]), standard[i], predictions[i], confidence[i]))\n",
    "    if not confidence:        \n",
    "        problematic_frame = pd.DataFrame(all_entries, columns=['Token', 'Context', 'Gold Standard', 'Prediction'])\n",
    "    else:\n",
    "        problematic_frame = pd.DataFrame(all_entries, columns=['Token', 'Context', 'Gold Standard', 'Prediction', 'Confidence'])\n",
    "    \n",
    "    return problematic_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50c90f",
   "metadata": {},
   "source": [
    "### EXECUTION - MODERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb8b53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens_upos, _ = extract_conllu_data(test_file, 'upos', sentences=True, combined=True)\n",
    "test_tokens_xpos, _ = extract_conllu_data(test_file, 'xpos', sentences=True, combined=True)\n",
    "test_tokens_lemmas, _ = extract_conllu_data(test_file, 'lemma', sentences=True, combined=True)\n",
    "\n",
    "# transforming it to a tagging-friendly format\n",
    "test_tokens, test_upos = make_tagger_friendly(test_tokens_upos)\n",
    "_, test_xpos = make_tagger_friendly(test_tokens_xpos)\n",
    "_, test_lemmas = make_tagger_friendly(test_tokens_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "394d8dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 14:46:58 INFO: Loading these models for language: pl (Polish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | pdb     |\n",
      "| mwt       | pdb     |\n",
      "| lemma     | pdb     |\n",
      "=======================\n",
      "\n",
      "2023-06-14 14:46:58 INFO: Use device: cpu\n",
      "2023-06-14 14:46:58 INFO: Loading: tokenize\n",
      "2023-06-14 14:46:58 INFO: Loading: mwt\n",
      "2023-06-14 14:46:58 INFO: Loading: lemma\n",
      "2023-06-14 14:46:58 INFO: Done loading processors!\n",
      "Retrieving annotations per sentence...: 100%|███████████████████████████████████████| 2215/2215 [00:23<00:00, 95.95it/s]\n"
     ]
    }
   ],
   "source": [
    "test_lemma_annotations = get_stanza_anns(test_tokens, 'tokenize,mwt,lemma', 'lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ff1406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 14:47:21 INFO: Loading these models for language: pl (Polish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | pdb     |\n",
      "| mwt       | pdb     |\n",
      "| pos       | pdb     |\n",
      "=======================\n",
      "\n",
      "2023-06-14 14:47:21 INFO: Use device: cpu\n",
      "2023-06-14 14:47:21 INFO: Loading: tokenize\n",
      "2023-06-14 14:47:21 INFO: Loading: mwt\n",
      "2023-06-14 14:47:22 INFO: Loading: pos\n",
      "2023-06-14 14:47:22 INFO: Done loading processors!\n",
      "Retrieving annotations per sentence...: 100%|███████████████████████████████████████| 2215/2215 [01:20<00:00, 27.42it/s]\n"
     ]
    }
   ],
   "source": [
    "test_upos_annotations = get_stanza_anns(test_tokens, 'tokenize,mwt,pos', 'upos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b63e2ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 14:48:43 INFO: Loading these models for language: pl (Polish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | pdb     |\n",
      "| mwt       | pdb     |\n",
      "| pos       | pdb     |\n",
      "=======================\n",
      "\n",
      "2023-06-14 14:48:43 INFO: Use device: cpu\n",
      "2023-06-14 14:48:43 INFO: Loading: tokenize\n",
      "2023-06-14 14:48:43 INFO: Loading: mwt\n",
      "2023-06-14 14:48:43 INFO: Loading: pos\n",
      "2023-06-14 14:48:43 INFO: Done loading processors!\n",
      "Retrieving annotations per sentence...: 100%|███████████████████████████████████████| 2215/2215 [01:16<00:00, 28.81it/s]\n"
     ]
    }
   ],
   "source": [
    "test_xpos_annotations = get_stanza_anns(test_tokens, 'tokenize,mwt,pos', 'xpos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37a45a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.89%\n"
     ]
    }
   ],
   "source": [
    "get_lemma_measures(test_lemmas, test_lemma_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2dcfc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.34%\n"
     ]
    }
   ],
   "source": [
    "get_lemma_measures(test_lemmas, test_lemma_annotations, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ddf89e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEASURES:\n",
      "Accuracy: 98.40%\n",
      "Precision (weighted): 98.41%\n",
      "Recall (weighted): 98.40%\n",
      "F1 (weighted): 98.40%\n",
      "Matthew's Correlation Coefficient: 98.16%\n",
      "\n",
      "MEASURES PER CLASS:\n",
      "Precision:\n",
      "\tADJ: 98.17%\n",
      "\tADP: 99.46%\n",
      "\tADV: 94.58%\n",
      "\tAUX: 95.44%\n",
      "\tCCONJ: 95.47%\n",
      "\tDET: 98.00%\n",
      "\tINTJ: 100.00%\n",
      "\tNOUN: 99.17%\n",
      "\tNUM: 98.48%\n",
      "\tPART: 95.01%\n",
      "\tPRON: 98.63%\n",
      "\tPROPN: 94.14%\n",
      "\tPUNCT: 99.95%\n",
      "\tSCONJ: 95.86%\n",
      "\tSYM: 100.00%\n",
      "\tVERB: 99.20%\n",
      "\tX: 93.53%\n",
      "Recall:\n",
      "\tADJ: 98.99%\n",
      "\tADP: 99.91%\n",
      "\tADV: 96.06%\n",
      "\tAUX: 97.14%\n",
      "\tCCONJ: 96.17%\n",
      "\tDET: 98.47%\n",
      "\tINTJ: 50.00%\n",
      "\tNOUN: 98.70%\n",
      "\tNUM: 98.11%\n",
      "\tPART: 90.97%\n",
      "\tPRON: 98.87%\n",
      "\tPROPN: 96.51%\n",
      "\tPUNCT: 99.95%\n",
      "\tSCONJ: 94.61%\n",
      "\tSYM: 25.00%\n",
      "\tVERB: 98.66%\n",
      "\tX: 93.53%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_measures(test_upos, test_upos_annotations, details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3a977d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEASURES:\n",
      "Accuracy: 94.29%\n",
      "Precision (weighted): 94.25%\n",
      "Recall (weighted): 94.29%\n",
      "F1 (weighted): 94.09%\n",
      "Matthew's Correlation Coefficient: 94.05%\n"
     ]
    }
   ],
   "source": [
    "get_measures(test_xpos, test_xpos_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf0db0",
   "metadata": {},
   "source": [
    "### EXECUTION - HISTORICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a43c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_upos, _ = extract_conllu_data(file_upos, 'upos', sentences=True, combined=True)\n",
    "tokens_xpos, _ = extract_conllu_data(file_xpos, 'xpos', sentences=True, combined=True)\n",
    "tokens_lemmas, _ = extract_conllu_data(file_xpos, 'lemma', sentences=True, combined=True)\n",
    "\n",
    "tokens_10k, upos = make_tagger_friendly(tokens_upos)\n",
    "tokens_3k, xpos = make_tagger_friendly(tokens_xpos)\n",
    "_, lemmas = make_tagger_friendly(tokens_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3badb896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 14:50:02 INFO: Loading these models for language: pl (Polish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | pdb     |\n",
      "| mwt       | pdb     |\n",
      "| lemma     | pdb     |\n",
      "=======================\n",
      "\n",
      "2023-06-14 14:50:02 INFO: Use device: cpu\n",
      "2023-06-14 14:50:02 INFO: Loading: tokenize\n",
      "2023-06-14 14:50:02 INFO: Loading: mwt\n",
      "2023-06-14 14:50:02 INFO: Loading: lemma\n",
      "2023-06-14 14:50:02 INFO: Done loading processors!\n",
      "Retrieving annotations per sentence...:  72%|██████████████████████████████▎           | 83/115 [00:01<00:00, 53.80it/s]"
     ]
    }
   ],
   "source": [
    "lemma_annotations = get_stanza_anns(tokens_3k, 'tokenize,mwt,lemma', 'lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd61daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "upos_annotations = get_stanza_anns(tokens_10k, 'tokenize,mwt,pos', 'upos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53366d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpos_annotations = get_stanza_anns(tokens_3k, 'tokenize,mwt,pos', 'xpos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lemma_measures(lemmas, lemma_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lemma_measures(lemmas, lemma_annotations, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lemmas = get_full_table(lemmas, lemma_annotations, tokens_3k)\n",
    "full_lemmas.to_excel('../data/results/stanza_lemmas.xlsx')\n",
    "\n",
    "full_lemmas_lowercase = get_full_table(lemmas, lemma_annotations, tokens_3k, lowercase=True)\n",
    "full_lemmas_lowercase.to_excel('../data/results/stanza_lowercase_lemmas.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = get_lemma_comparison(lemmas, lemma_annotations, tokens_3k)\n",
    "comparison.to_excel('../data/mistakes/stanza_lemma_mistakes.xlsx')\n",
    "\n",
    "comparison_lowercase = get_lemma_comparison(lemmas, lemma_annotations, tokens_3k, lowercase=True)\n",
    "comparison_lowercase.to_excel('../data/mistakes/stanza_lowercase_lemma_mistakes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faaab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_measures(upos, upos_annotations, details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_upos = get_full_table(upos, upos_annotations, tokens_10k)\n",
    "full_upos.to_excel('../data/results/stanza_upos.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8282ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "upos_comparison = get_comparison(upos, upos_annotations, tokens_10k)\n",
    "upos_comparison.to_excel('../data/mistakes/stanza_UPOS_mistakes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e81db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "upos_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479eb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_measures(xpos, xpos_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336013a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_xpos = get_full_table(xpos, xpos_annotations, tokens_3k)\n",
    "full_xpos.to_excel('../data/results/stanza_xpos.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpos_comparison = get_comparison(xpos, xpos_annotations, tokens_3k)\n",
    "xpos_comparison.to_excel('../data/mistakes/stanza_XPOS_mistakes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpos_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57b646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
